{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"font-family:'arial';font-size:25px\"> Set up for batch-style computing on the *All of Us* Researcher Workbench with dsub </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cell below provides the environment information, time and cost to run the notebook. For this tutorial, your cloud analysis environment can be left with the default settings for a General Analyses.\n",
    "- This notebook only takes a few minutes to run interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Cloud Analysis Environment</b>: Use \"Recommended Environment\" <kbd><b>General Analysis</b></kbd> which creates compute type <kbd><b>Standard VM</b></kbd> with default values of 4 CPUs, 15GB RAM, and 120GB disk.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "We recommend that researchers use this notebook to learn the basics of using dsub in the *All of Us* Research Workbench and with Genomic Data. This notebook will show how to set up [dsub](https://github.com/databiosphere/dsub) for use on the *All of Us* Researcher Workbench.\n",
    "\n",
    "**What you will learn:**\n",
    "1. What is dsub?\n",
    "1. When would I want to use dsub instead of a notebook?\n",
    "1. How to install dsub.\n",
    "1. How to create a bash function with default argument values for dsub.\n",
    "\n",
    "\n",
    "See also the [dsub documentation](https://github.com/databiosphere/dsub#dsub-features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is dsub?\n",
    "\n",
    "dsub is a command-line tool that makes it easy to submit and run batch scripts in the cloud.\n",
    "\n",
    "The dsub user experience is modeled after traditional high-performance computing job schedulers like Grid Engine and Slurm. You write a script and then submit it to a job scheduler from a shell prompt on your local machine.\n",
    "\n",
    "See also the [dsub documentation](https://github.com/databiosphere/dsub#dsub-features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When would I want to use dsub?\n",
    "\n",
    "You can use `%%bash` or `!` in a notebook to run command line tools like `plink`. It works fine, and it's nice to show your work in a [literate programming style](https://en.wikipedia.org/wiki/Literate_programming)! You can of course also use the Jupyter terminal to run command line tools like `plink`.\n",
    "\n",
    "\n",
    "You might prefer to run those command line tools via scripts with dsub when you want to:\n",
    "* run the script in **parallel** (e.g., to process data for different chromosomes on different machines simultaneously)\n",
    "* run the script **on a different machine** than where Jupyter is running (e.g., so that CPU and RAM are dedicated, not shared)\n",
    "* run something that may take **longer than 24 hours** (e.g., to avoid cloud analysis environment [autopause](https://support.terra.bio/hc/en-us/articles/360029761352-Preventing-runaway-costs-with-notebook-auto-pause-#h_de5698f5-3c82-4763-aaaf-ea7df6a1869c))\n",
    "* run something using **inexpensive [preemptible VMs](https://cloud.google.com/compute/docs/instances/preemptible)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dsub in /home/jupyter/.local/lib/python3.10/site-packages (0.4.12)\n",
      "Requirement already satisfied: funcsigs==1.0.2 in /opt/conda/lib/python3.10/site-packages (from dsub) (1.0.2)\n",
      "Requirement already satisfied: google-api-core<=2.19.0,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.11.1)\n",
      "Requirement already satisfied: google-api-python-client<=2.127.0,>=2.47.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.127.0)\n",
      "Requirement already satisfied: google-auth-httplib2<=0.2.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.2.0)\n",
      "Requirement already satisfied: google-auth<=2.29.0,>=2.6.6 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-batch<=0.17.18 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.17.18)\n",
      "Requirement already satisfied: httplib2<=0.22.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.21.0)\n",
      "Requirement already satisfied: mock<=5.1.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (5.1.0)\n",
      "Requirement already satisfied: parameterized<=0.9.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.9.0)\n",
      "Requirement already satisfied: pyasn1-modules<=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.2.7)\n",
      "Requirement already satisfied: pyasn1<=0.6.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.4.8)\n",
      "Requirement already satisfied: python-dateutil<=2.9.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.8.2)\n",
      "Requirement already satisfied: pytz<=2024.1 in /opt/conda/lib/python3.10/site-packages (from dsub) (2023.3)\n",
      "Requirement already satisfied: pyyaml<=6.0.1 in /opt/conda/lib/python3.10/site-packages (from dsub) (6.0.1)\n",
      "Requirement already satisfied: rsa<=4.9 in /opt/conda/lib/python3.10/site-packages (from dsub) (4.9)\n",
      "Requirement already satisfied: tabulate<=0.9.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.9.0)\n",
      "Requirement already satisfied: tenacity<=8.2.3 in /opt/conda/lib/python3.10/site-packages (from dsub) (8.2.3)\n",
      "Requirement already satisfied: uritemplate<=4.1.1 in /opt/conda/lib/python3.10/site-packages (from dsub) (3.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<=2.19.0,>=2.7.3->dsub) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core<=2.19.0,>=2.7.3->dsub) (3.19.6)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core<=2.19.0,>=2.7.3->dsub) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<=2.29.0,>=2.6.6->dsub) (4.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<=2.29.0,>=2.6.6->dsub) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<=2.29.0,>=2.6.6->dsub) (1.26.15)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-batch<=0.17.18->dsub) (1.22.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<=0.22.0->dsub) (3.0.9)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-batch<=0.17.18->dsub) (1.48.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-batch<=0.17.18->dsub) (1.48.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<=2.19.0,>=2.7.3->dsub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<=2.19.0,>=2.7.3->dsub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<=2.19.0,>=2.7.3->dsub) (2024.2.2)\n",
      "\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.8.5.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade dsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View `dsub --help`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dsub has many parameters. It is a really flexible system, but for several of the parameter values, we either always want to pass the same value when running on the *All of Us* Researcher Workbench, or there is a recommended default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: /opt/conda/bin/dsub [-h] [--provider PROVIDER] [--version VERSION]\n",
      "                           [--unique-job-id] [--name NAME]\n",
      "                           [--tasks [FILE M-N ...]] [--image IMAGE]\n",
      "                           [--dry-run] [--command COMMAND] [--script SCRIPT]\n",
      "                           [--env [KEY=VALUE ...]] [--label [KEY=VALUE ...]]\n",
      "                           [--input [KEY=REMOTE_PATH ...]]\n",
      "                           [--input-recursive [KEY=REMOTE_PATH ...]]\n",
      "                           [--output [KEY=REMOTE_PATH ...]]\n",
      "                           [--output-recursive [KEY=REMOTE_PATH ...]]\n",
      "                           [--user USER] [--user-project USER_PROJECT]\n",
      "                           [--mount [KEY=PATH_SPEC ...]] [--wait]\n",
      "                           [--retries RETRIES] [--poll-interval POLL_INTERVAL]\n",
      "                           [--after AFTER [AFTER ...]] [--skip] [--summary]\n",
      "                           [--min-cores MIN_CORES] [--min-ram MIN_RAM]\n",
      "                           [--disk-size DISK_SIZE] [--logging LOGGING]\n",
      "                           [--project PROJECT]\n",
      "                           [--boot-disk-size BOOT_DISK_SIZE]\n",
      "                           [--preemptible [PREEMPTIBLE]]\n",
      "                           [--zones ZONES [ZONES ...]]\n",
      "                           [--scopes SCOPES [SCOPES ...]]\n",
      "                           [--accelerator-type ACCELERATOR_TYPE]\n",
      "                           [--accelerator-count ACCELERATOR_COUNT]\n",
      "                           [--credentials-file CREDENTIALS_FILE]\n",
      "                           [--regions REGIONS [REGIONS ...]]\n",
      "                           [--machine-type MACHINE_TYPE]\n",
      "                           [--cpu-platform CPU_PLATFORM] [--network NETWORK]\n",
      "                           [--subnetwork SUBNETWORK] [--use-private-address]\n",
      "                           [--timeout TIMEOUT] [--log-interval LOG_INTERVAL]\n",
      "                           [--ssh] [--service-account SERVICE_ACCOUNT]\n",
      "                           [--disk-type DISK_TYPE]\n",
      "                           [--enable-stackdriver-monitoring]\n",
      "                           [--block-external-network] [--location LOCATION]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --provider PROVIDER   Job service provider. Valid values are \"google-v2\"\n",
      "                        (Google's Pipeline API v2alpha1), \"google-cls-v2\"\n",
      "                        (Google's Pipelines API v2beta), \"google-batch\"\n",
      "                        (Google's Batch API v1alpha1), and \"local\" (local\n",
      "                        Docker execution). \"test-*\" providers are for testing\n",
      "                        purposes only. (default: google-v2)\n",
      "  --version VERSION, -v VERSION\n",
      "                        Print the dsub version and exit.\n",
      "  --unique-job-id       Experimental: create a unique 32 character UUID for\n",
      "                        the dsub job-id using\n",
      "                        https://docs.python.org/3/library/uuid.html. (default:\n",
      "                        False)\n",
      "  --name NAME           Name for the job. Defaults to the script name or first\n",
      "                        token of the --command if specified.\n",
      "  --tasks [FILE M-N ...]\n",
      "                        Path to a file of tab separated values (TSV) for task\n",
      "                        parameters. The file may be located in the local\n",
      "                        filesystem or in a Google Cloud Storage bucket. The\n",
      "                        first line is a list of column headers specifying an\n",
      "                        --env, --input, --input-recursive, --output or\n",
      "                        --output-recursive variable, and each subsequent line\n",
      "                        specifies the values for a task. Optionally specify\n",
      "                        tasks from the file to submit. Can take the form \"m\",\n",
      "                        \"m-\", or \"m-n\" where m and n are task numbers starting\n",
      "                        at 1. (default: None)\n",
      "  --image IMAGE         Image name from Docker Hub, Google Container\n",
      "                        Repository, or other Docker image service. The task\n",
      "                        must have READ access to the image. (default:\n",
      "                        ubuntu:14.04)\n",
      "  --dry-run             Print the task(s) that would be run and then exit.\n",
      "                        (default: False)\n",
      "  --command COMMAND     Command to run inside the job's Docker container. This\n",
      "                        argument or the --script argument must be provided.\n",
      "  --script SCRIPT       Path to a script that is located in the local file\n",
      "                        system or inside a Google Cloud Storage bucket. This\n",
      "                        script will be run inside the job's Docker container.\n",
      "                        This argument or the --command argument must be\n",
      "                        provided.\n",
      "  --env [KEY=VALUE ...]\n",
      "                        Environment variables for the script's execution\n",
      "                        environment\n",
      "  --label [KEY=VALUE ...]\n",
      "                        Labels to associate to the job.\n",
      "  --input [KEY=REMOTE_PATH ...]\n",
      "                        Input path arguments to localize into the script's\n",
      "                        execution environment\n",
      "  --input-recursive [KEY=REMOTE_PATH ...]\n",
      "                        Input path arguments to localize recursively into the\n",
      "                        script's execution environment\n",
      "  --output [KEY=REMOTE_PATH ...]\n",
      "                        Output path arguments to de-localize from the script's\n",
      "                        execution environment\n",
      "  --output-recursive [KEY=REMOTE_PATH ...]\n",
      "                        Output path arguments to de-localize recursively from\n",
      "                        the script's execution environment\n",
      "  --user USER, -u USER  User submitting the dsub job, defaults to the current\n",
      "                        OS user.\n",
      "  --user-project USER_PROJECT\n",
      "                        Specify a user project to be billed for all requests\n",
      "                        to Google Cloud Storage (logging, localization,\n",
      "                        delocalization, mounting). This flag exists to support\n",
      "                        accessing Requester Pays buckets (default: None)\n",
      "  --mount [KEY=PATH_SPEC ...]\n",
      "                        Mount a resource such as a bucket, disk, or directory\n",
      "                        into your Docker container\n",
      "  --wait                Wait for the job to finish all its tasks. (default:\n",
      "                        False)\n",
      "  --retries RETRIES     Number of retries to perform on failed tasks.\n",
      "                        (default: 0)\n",
      "  --poll-interval POLL_INTERVAL\n",
      "                        Polling interval (in seconds) for checking job status\n",
      "                        when --wait or --after are set. (default: 10)\n",
      "  --after AFTER [AFTER ...]\n",
      "                        Job ID(s) to wait for before starting this job.\n",
      "  --skip                Do not submit the job if all output specified using\n",
      "                        the --output and --output-recursive parameters already\n",
      "                        exist. Note that wildcard and recursive outputs cannot\n",
      "                        be strictly verified. See the documentation for\n",
      "                        details. (default: False)\n",
      "  --summary             During the --wait loop, display a summary of the\n",
      "                        results, grouped by (job, status). (default: False)\n",
      "  --min-cores MIN_CORES\n",
      "                        Minimum CPU cores for each job. The default is\n",
      "                        provider-specific. The google-v2 provider default is 1\n",
      "                        core. The local provider does not allocate resources,\n",
      "                        but uses available resources of your machine.\n",
      "  --min-ram MIN_RAM     Minimum RAM per job in GB. The default is provider-\n",
      "                        specific. The google-v2 provider default is 3.75 GB.\n",
      "                        The local provider does not allocate resources, but\n",
      "                        uses available resources of your machine.\n",
      "  --disk-size DISK_SIZE\n",
      "                        Size (in GB) of data disk to attach for each job\n",
      "                        (default: 200)\n",
      "  --logging LOGGING     Cloud Storage path to send logging output (either a\n",
      "                        folder, or file ending in \".log\")\n",
      "\n",
      "google-common:\n",
      "  Options common to the \"google-cls-v2\", \"google-v2\" and\n",
      "          \"google-batch\" providers\n",
      "\n",
      "  --project PROJECT     Cloud project ID in which to run the job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --boot-disk-size BOOT_DISK_SIZE\n",
      "                        Size (in GB) of the boot disk (default: 10, 30 for\n",
      "                        google-batch)\n",
      "  --preemptible [PREEMPTIBLE]\n",
      "                        If --preemptible is given without a number, enables\n",
      "                        preemptible VMs for all attempts for all tasks. If a\n",
      "                        number value N is used, enables preemptible VMs for up\n",
      "                        to N attempts for each task. Defaults to not using\n",
      "                        preemptible VMs.\n",
      "  --zones ZONES [ZONES ...]\n",
      "                        List of Google Compute Engine zones.\n",
      "  --scopes SCOPES [SCOPES ...]\n",
      "                        Space-separated scopes for Google Compute Engine\n",
      "                        instances. If unspecified, provider will use 'https://\n",
      "                        www.googleapis.com/auth/bigquery,https://www.googleapi\n",
      "                        s.com/auth/compute,https://www.googleapis.com/auth/dev\n",
      "                        storage.full_control,https://www.googleapis.com/auth/g\n",
      "                        enomics,https://www.googleapis.com/auth/logging.write,\n",
      "                        https://www.googleapis.com/auth/monitoring.write'\n",
      "  --accelerator-type ACCELERATOR_TYPE\n",
      "                        The Compute Engine accelerator type. See\n",
      "                        https://cloud.google.com/compute/docs/gpus/ for\n",
      "                        supported GPU types. Only NVIDIA GPU accelerators are\n",
      "                        currently supported. If an NVIDIA GPU is attached, the\n",
      "                        required runtime libraries will be made available to\n",
      "                        all containers under /usr/local/nvidia. Each version\n",
      "                        of Container-Optimized OS image (used by the Pipelines\n",
      "                        API) has a default supported NVIDIA GPU driver\n",
      "                        version. See https://cloud.google.com/container-\n",
      "                        optimized-os/docs/how-to/run-gpus#install Note that\n",
      "                        attaching a GPU increases the worker VM startup time\n",
      "                        by a few minutes. (default: None)\n",
      "  --accelerator-count ACCELERATOR_COUNT\n",
      "                        The number of accelerators of the specified type to\n",
      "                        attach. By specifying this parameter, you will\n",
      "                        download and install the following third-party\n",
      "                        software onto your job's Compute Engine instances:\n",
      "                        NVIDIA(R) Tesla(R) drivers and NVIDIA(R) CUDA toolkit.\n",
      "                        (default: 0)\n",
      "  --credentials-file CREDENTIALS_FILE\n",
      "                        Path to a local file with JSON credentials for a\n",
      "                        service account.\n",
      "  --regions REGIONS [REGIONS ...]\n",
      "                        List of Google Compute Engine regions. Only one of\n",
      "                        --zones and --regions may be specified.\n",
      "  --machine-type MACHINE_TYPE\n",
      "                        Provider-specific machine type (default: None)\n",
      "  --cpu-platform CPU_PLATFORM\n",
      "                        The CPU platform to request. Supported values can be\n",
      "                        found at https://cloud.google.com/compute/docs/instanc\n",
      "                        es/specify-min-cpu-platform (default: None)\n",
      "  --network NETWORK     The Compute Engine VPC network name to attach the VM's\n",
      "                        network interface to. The value will be prefixed with\n",
      "                        global/networks/ unless it contains a /, in which case\n",
      "                        it is assumed to be a fully specified network resource\n",
      "                        URL. (default: None)\n",
      "  --subnetwork SUBNETWORK\n",
      "                        The name of the Compute Engine subnetwork to attach\n",
      "                        the instance to. (default: None)\n",
      "  --use-private-address\n",
      "                        If set to true, do not attach a public IP address to\n",
      "                        the VM. (default: False)\n",
      "  --timeout TIMEOUT     The maximum amount of time to give the task to\n",
      "                        complete. This includes the time spent waiting for a\n",
      "                        worker to be allocated. Time can be listed using a\n",
      "                        number followed by a unit. Supported units are s\n",
      "                        (seconds), m (minutes), h (hours), d (days), w\n",
      "                        (weeks). The provider-specific default is 7 days.\n",
      "                        Example: '7d' (7 days).\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        The amount of time to sleep between copies of log\n",
      "                        files from the task to the logging path. Time can be\n",
      "                        listed using a number followed by a unit. Supported\n",
      "                        units are s (seconds), m (minutes), h (hours).\n",
      "                        Example: '5m' (5 minutes). Default is '1m'.\n",
      "  --ssh                 If set to true, start an ssh container in the\n",
      "                        background to allow you to log in using SSH and debug\n",
      "                        in real time. (default: False)\n",
      "  --service-account SERVICE_ACCOUNT\n",
      "                        Email address of the service account to be authorized\n",
      "                        on the Compute Engine VM for each job task. If not\n",
      "                        specified, the default Compute Engine service account\n",
      "                        for the project will be used.\n",
      "  --disk-type DISK_TYPE\n",
      "                        The disk type to use for the data disk. Valid values\n",
      "                        are pd-standard pd-ssd and local-ssd. The default\n",
      "                        value is pd-standard.\n",
      "  --enable-stackdriver-monitoring\n",
      "                        If set to true, enables Stackdriver monitoring on the\n",
      "                        VM. (default: False)\n",
      "  --block-external-network\n",
      "                        If set to true, prevents the container for the user's\n",
      "                        script/command from accessing the external network.\n",
      "                        (default: False)\n",
      "\n",
      "\"google-cls-v2\" provider options:\n",
      "  See also the \"google-common\" options listed above\n",
      "\n",
      "  --location LOCATION   Specifies the Google Cloud region to which the\n",
      "                        pipeline request will be sent and where operation\n",
      "                        metadata will be stored. The associated dsub task may\n",
      "                        be executed in another region if the --regions or\n",
      "                        --zones arguments are specified. (default: us-\n",
      "                        central1)\n",
      "\n",
      "Provider-required arguments:\n",
      "  google-batch: ['project', 'logging']\n",
      "  google-cls-v2: ['project', 'logging']\n",
      "  google-v2: ['project', 'logging']\n",
      "  test-fails: []\n",
      "  local: ['logging']\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dsub --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup `aou_dsub` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers can avoid a lot of unnecessary typing of those default parameter values by using this [bash function](https://linuxize.com/post/bash-functions/) to call dsub.\n",
    "\n",
    "Use of the `aou_dsub` function is optional but recommended because, in addition to removing a lot of boilerplate code, it also creates a nice folder structure for dsub log files. Feel free to customize it to meet your needs.\n",
    "\n",
    "You can use this `aou_dsub` function from both within `%%bash` cells in notebooks and also from the Jupyter terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jupyter/aou_dsub.bash\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/aou_dsub.bash\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# This shell function passes reasonable defaults for several dsub parameters, while\n",
    "# allowing the caller to override any of them. It creates a nice folder structure within\n",
    "# the workspace bucket for dsub log files.\n",
    "\n",
    "# --[ Parameters ]--\n",
    "# any valid dsub parameter flag\n",
    "\n",
    "#--[ Returns ]--\n",
    "# the job id of the job created by dsub\n",
    "\n",
    "#--[ Details ]--\n",
    "# The first five parameters below should always be those values when running on AoU RWB.\n",
    "\n",
    "# Feel free to change the values for --user, --regions, --logging, and --image if you like.\n",
    "\n",
    "# Note that we insert some job data into the logging path.\n",
    "# https://github.com/DataBiosphere/dsub/blob/main/docs/logging.md#inserting-job-data\n",
    "\n",
    "function aou_dsub () {\n",
    "\n",
    "  # Get a shorter username to leave more characters for the job name.\n",
    "  local DSUB_USER_NAME=\"$(echo \"${OWNER_EMAIL}\" | cut -d@ -f1)\"\n",
    "\n",
    "  # For AoU RWB projects network name is \"network\".\n",
    "  local AOU_NETWORK=network\n",
    "  local AOU_SUBNETWORK=subnetwork\n",
    "\n",
    "  dsub \\\n",
    "      --provider google-cls-v2 \\\n",
    "      --user-project \"${GOOGLE_PROJECT}\"\\\n",
    "      --project \"${GOOGLE_PROJECT}\"\\\n",
    "      --image 'marketplace.gcr.io/google/ubuntu1804:latest' \\\n",
    "      --network \"${AOU_NETWORK}\" \\\n",
    "      --subnetwork \"${AOU_SUBNETWORK}\" \\\n",
    "      --service-account \"$(gcloud config get-value account)\" \\\n",
    "      --user \"${DSUB_USER_NAME}\" \\\n",
    "      --regions us-central1 \\\n",
    "      --logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +'%Y%m%d/%H%M%S')/{job-id}-{task-id}-{task-attempt}.log\" \\\n",
    "      \"$@\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make `aou_dsub` available from the terminal too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add this function to `.bashrc` so that we can easily use it from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo source ~/aou_dsub.bash >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run all the cells in this notebook, open a **new** terminal and run:\n",
    "```\n",
    "aou_dsub --help | more\n",
    "```\n",
    "\n",
    "[If you run this in an existing terminal and get error `aou_dsub: command not found`, just run `source ~/.bashrc` first.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsub==0.4.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip3 freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.766px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
